{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Design of Experiment for Adaptive Sampling\n",
    "## A look at online information gathering\n",
    "\n",
    "For this PSet we'll be walking you through the core of adaptive sampling theory -- namely we'll be having you react to observations made in an unknown environment in order to best address a goal of your robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: Modeling as a GP and Information Theory\n",
    "\n",
    "First, we'll be asking you to review a little bit about Gaussian Processes (GPs) in order to understand how we'll be asking you to model your probabilistic belief in the world as you navigate and take samples. We will also present a primer on information-theoretic measures to set you up for starting to formulate the relevant functions necessary to motivate the adaptive sampling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with your world representation. In many adaptive sampling tasks, the robot has little to no information about the environment of interest. As samples are collected, the robot should be able to start modeling the environment probabilistically in order to inform the next trajectories. As we learned in a previous lecture, GPs are a particualrly compelling technique for modeling distributions of a variable of interest when we are not interested or are not able to define specific parameters. We're asking you to keep a map of your belief through a GP representation for this exercise. \n",
    "\n",
    "We have provided a helpful library for creating, updating, and plotting your GP belief map. Let's get familiar with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "# instantiate a test GP, provide it a set of data, and plot these quantities as an example for the\n",
    "# student to get familiar with the API of the GP library we have selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn. Please complete the function which takes as input a GP instance and a sample of data, and returns an updated GP instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_belief(GP, sample):\n",
    "    \"\"\"\n",
    "    input: \n",
    "    GP -- a GP object\n",
    "    sample -- data with information that can be used to update the GP object\n",
    "    \n",
    "    output: updated GP object\n",
    "    \"\"\"\n",
    "    # TODO your code here!\n",
    "    raise NotImplementedError('Solution not yet implemented')\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that it works with a simple fitting problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "#instantiate a GP instance\n",
    "#have a list of data\n",
    "#iterate through the list of data and use the update_belief function\n",
    "#check that it matches the expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're ready to start collecting belief, let's introduce you to a few ways of defining \"information\"....[INSERT DISCUSSION ABOUT INFORMATION HERE].\n",
    "\n",
    "[HAVE STUDENTS IMPLEMENT A FEW MEASURES OF \"INFORMATION\" ON A SET OF DATA AND COMPARE THOSE MEASURES]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "#walk through of implementing information measures (provide a set of data to look at)\n",
    "#help them visualize/compare these measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You're now ready to start formulating the goal for your vehicle and compelling it to start adaptive sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressing Reward and Cost\n",
    "\n",
    "Central to the adaptive sampling problem is expressing an objective (generally maximizing information gain or minimizing uncertainty) and formulating an appropriate reward and cost function for the vehicle. These functions are ultimately applied to all possible actions in order to choose the \"best\" action at that time.\n",
    "\n",
    "For this PSet, we would like you to work with this objective:\n",
    "\n",
    "$$ arg \\max\\limits_{r \\in S} I(r) \\quad \\textrm{s.t.} \\quad C(r) \\leq B $$\n",
    "\n",
    "That is, from all possible actions, choose the action which maximizes information along that action; subject to a budgetary constraint applied to the cost of the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Rewarding in Adaptive Sampling?\n",
    "\n",
    "Depending on the specific application of this adaptive sampling platform, there may be several different ways of formulating this reward. Let's look at three of them:\n",
    "\n",
    "**Exploitative -- High Values** -- Sometimes all we care about is collecting samples that are \"interesting\" because they have a high cost. One might think about the case of sampling in the ocean, and only focusing on areas with dissolved CO2 or methane higher than some baseline threshold). This is known as an exploitative reward. Write a function which takes in a series of sample measurements and selects the most exploitative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_exploitative_reward(samples):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    samples -- a list of samples (include location and mean/variance of the sample)\n",
    "    \n",
    "    output:\n",
    "    return the location of the most exploitative sample\n",
    "    \"\"\"\n",
    "    #TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### TODO \n",
    "#test their function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explorative -- Coverage is Key** -- Now we consider the case in which we only care about collecting samples which we have low certainty on. Imagine a scenario in which all we want is a good survey of a large area -- this reward should help to optimize a path which \"sees\" a lot of this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_explorative_reward(samples):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    samples -- a list of samples (includes location and mean/variance of the sample)\n",
    "    \n",
    "    output:\n",
    "    returns the location of the most explorative sample\n",
    "    \"\"\"\n",
    "    #TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "#test their function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trading off** -- Sometimes we actually care about both exploring in high value areas and seeing a lot of the world. This is the \"explore-exploit\" tradeoff, and this is well discussed in the literature. Often the elements of explore are weighted by some parameter $\\beta$. Please write a function that takes in a set of samples and some $\\beta$ and returns the best trade-off value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_explore_exploit_reward(samples, beta):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    samples -- a list of samples (includes location and mean/variance of the sample)\n",
    "    beta -- a parameter which weights the importance of exploring\n",
    "    \n",
    "    output:\n",
    "    returns the location of the best trade-off sample\n",
    "    \"\"\"\n",
    "    #TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "#test their function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How costly is sampling?\n",
    "\n",
    "Finally, we want to also be able to quantify how costly taking a certain sample is. For the purposes of this PSet, let's define cost as \"cost to go\" -- that is, the euclidean distance between the pose of the vehicle and the location of the sample. Please complete the cost function below with this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(pose, sample):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    pose -- location of the vehicle\n",
    "    sample -- a sample that needs to be assessed; includes location information\n",
    "    \n",
    "    output:\n",
    "    returns the cost-to-go\n",
    "    \"\"\"\n",
    "    #TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### TODO\n",
    "#test their function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Around: Navigating based on Sample Utility\n",
    "\n",
    "Now that we have formulated our reward and cost functions, let's think about how we'll apply them to getting around a world. We've endowed your robot with the ability to go forward, back, right, and left. These are \"action primitives\" in this world, and you can think of these as $S$ in your objective formulation. In order to select the best action at each time step, you will need to apply your reward and cost functions to each of these actions to assess two things: that the action has the best reward, and that the action is \"within budget.\" \n",
    "\n",
    "Complete the function below that will take in your current location, your action primitives, and your GP in order to pick the best primitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_primitive(pose, belief, budget, primitive_set, reward_function='exploit'):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    pose -- robot's current position in the world\n",
    "    belief -- GP instance\n",
    "    budget -- the budget that is left for sampling\n",
    "    primitive_set -- the actions which can be taken (represented as step-sizes in the world)\n",
    "    reward_function -- keyword for which reward function to be used\n",
    "    \"\"\"\n",
    "    # TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### TODO\n",
    "#test their function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it All Together: Exploring Europa\n",
    "\n",
    "You've just joined the engineering team responsible for determining how one of the rovers on the next trip to Europa should be collecting samples. This is a complicated task -- we don't have a great sense of the type of environment that the rover will be landing in, though we know that we are generally interested in the density of carbon in ice samples taken from the surface. As sampling is the only way we can learn about this icy sphere, it is critical that the samples collected are somewhat representative of the area that the rover has landed in. As the rover has a number of other missions it needs to conduct, your team has been given a budget of how much the vehicle can travel before the next mission starts. \n",
    "\n",
    "Finish your work so far by allowing the utility assessment to operate over multiple timesteps until the budget is depleted. Return the total reward you've collected, the series of actions you've taken, and the final belief map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_europa(budget, primitive_set, reward_type=\"exploit\", beta=0):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    budget -- the total travel budget allowed for your mission\n",
    "    primitive_set -- the known ways the robot can move at each time step\n",
    "    reward_type -- determine which reward function to use\n",
    "    beta -- relevant to if the exploit-explore reward is used\n",
    "    \n",
    "    output:\n",
    "    total_reward -- how much reward was accumulated during the search\n",
    "    actions_taken -- list of all of the decisions\n",
    "    final_map -- final GP instance\n",
    "    \"\"\"\n",
    "    # TODO your code here!\n",
    "    raise NotImplementedError(\"Not yet implemented\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### TODO\n",
    "# create several test scenarios for their code....maybe have them do this part and then discuss what changes\n",
    "# when parameters are changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Open Areas -- How Much is Enough?\n",
    "\n",
    "What did you notice when you adjusted the reward function you used? Or beta? What are the limitations of using a pre-specified budget value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here, Please*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "You've now successfully implemented the core pipeline for adaptive sampling. There is a LOT of literature out there which explores this problem in a number of other contexts: active SLAM, active learning, sensor selection, informative path planning, the multi-armed bandit problem, etc. In nearly any online type setting in robotics, being able to adapt to one's environment is critical to success of the platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
